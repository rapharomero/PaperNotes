{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Random Graphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random graphs is a well studied field that can be applied to the analysis of large networks.\n",
    "It can be used for instance to model the interactions between entities in a social network, songs in a music database, or to study biological networks.  \n",
    "Exponential random graphs is a large family of model that can capture various kind of structural properties of the networks. \n",
    "Here a brief introduction to these models is provided.\n",
    "\n",
    "source: https://arxiv.org/pdf/cond-mat/0405566.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum entropy distribution over graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many real-world problem settings, one does not have access to a whole network entirely, but only to a set of statistics of the graph, where can be thought of as *observables* in the language of statistical physics.  \n",
    "\n",
    "Let's denote these observables $\\{\\bar{x_i}\\},  i=1...r$  \n",
    "The goal is to derive a distribution over the set of graphs, that would satisfy these observations.  \n",
    "In order to minimize the additional information provided by the model given these observation, a natural way to model the underlying network is to derive the distribution that has maximal entropy, among all the possible graph distributions.\n",
    "\n",
    "This amounts in solving the following maximum entropy problem: \n",
    "$$ S(P)=-\\sum_{G \\in \\mathcal{G}} P(G) log(G)) $$ \n",
    "\n",
    "$$ max_{P}S(P)$$\n",
    "s.t.\n",
    "$$ \\forall i \\sum_{G \\in \\mathcal{G}} P(G)x_i(G)= \\bar{x_i}$$ and\n",
    "$$ \\sum_{G \\in \\mathcal{G}} P(G) = 1$$\n",
    "\n",
    "\n",
    "This is equivalent to $$\\frac{\\partial}{\\partial(P(G))}[S + \\alpha(1 - \\sum_G P(G)) + \\sum_i(\\theta_i(\\bar{x_i} - \\sum_G P(G)x_i(G)))] = 0$$\n",
    "\n",
    "i.e \n",
    "$$ lnP(G) + 1 + \\alpha + \\sum_i \\theta_i x_i(G) = 0$$\n",
    "\n",
    "This leads to the following general formula for the maximum-entropy distribution: \n",
    "$$P(G) = \\frac{e^{-H(G)}}{Z}$$\n",
    "\n",
    "\n",
    "Where we introduce the **Hamiltonian** associated with the constraints : $$ H(G) = \\sum_i \\theta_i x_i(G)$$\n",
    "\n",
    "and the **Partition Function** $$Z = e^{1+\\alpha} = \\sum_G e^{-H(G)}$$\n",
    "\n",
    "The **Free Energy** is defined as $$F = -ln Z$$ \n",
    "\n",
    "This free energy plays an important role since its derivative w.r.t to each lagrange multiplier is equal to the associated observable.  \n",
    "Indeed we have \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial F}{\\partial \\theta_i} &= - \\frac{1}{Z} \\frac{\\partial Z}{\\partial \\theta_i} \\\\\n",
    "                                    &= \\frac{1}{Z}\\sum_G x_i(G) e^{-H(G)} \\\\\n",
    "                                    &= \\sum_G P(G) x_i(G) \\\\\n",
    "                                    &= \\mathbb{E}_P[x_i(G)]\n",
    "\\end{align}                                    \n",
    "$$\n",
    "\n",
    "Its higher order derivatives can be used to express higher order statistics of the observables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Exponential random graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen before, an exponential random graph is totally determined by the set of constraints under which the maximum entropy problem is solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Number of edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of simple unoriented graphs a simple constraint that can be imposed to the graph can be its total number of edges $\\bar{m}$.\n",
    "\n",
    "Let $A=(a_{ij})$ be the adjacency matrix of the graph, this constraint can be expressed as \n",
    "$$m(G) = \\sum_{i<j} a_{ij} = \\bar{m}$$\n",
    "\n",
    "The hamiltonian is then $H(G) = \\theta m(G)$ (only one constraint).\n",
    "\n",
    "The partition function is \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    Z &= \\sum_G e^{-H(G)} = \\sum_{A \\in \\{0,1\\}^{n^2}} exp({-\\theta \\sum_{i<j} a_{ij}}) \\\\  \n",
    "      &= \\prod_{i<j} \\sum_{a_{ij}\\in \\{0,1\\}} exp(-\\theta a_{ij}) \\\\\n",
    "      &= \\prod_{i<j} (1 + e^{-\\theta}) \\\\\n",
    "      &= (1 + e^{-\\theta})^{n \\choose 2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The exchange between the product and sum is a consequence of the following algebraic relation:  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{(u_1...u_n) \\in \\{0;1\\}^n} exp(-\\theta\\sum_{i} u_i) \n",
    "&= \\sum_{u_1 \\in \\{0;1\\}}\\sum_{u_2 \\in \\{0;1\\}} ... \\sum_{u_n \\in \\{0;1\\}}  exp(-\\theta u_1)exp(-\\theta u_2) ...exp(-\\theta u_n) \\\\\n",
    " &= (\\sum_{u_1 \\in \\{0;1\\}} exp(-\\theta u_1))(\\sum_{u_2 \\in \\{0;1\\}} exp(-\\theta u_2)) ... (\\sum_{u_n \\in \\{0;1\\}} exp(-\\theta u_n)) \\\\\n",
    " &= \\prod_{i \\in {1...n}} (\\sum_{(u_i) \\in \\{0;1\\}} exp(-\\theta u_i))\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "\n",
    "\n",
    "The Free energy is : \n",
    "$$ \n",
    "\\begin{align}\n",
    "    F &= {n\\choose 2} ln(1 + e^{-\\theta})\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So the expected number of edges with respect to the max-ent model is \n",
    "$$\\bar{m} = {n\\choose 2} \\frac{1}{(1 + e^{-\\theta})} $$\n",
    "So the maximum entropy distribution in this case is \n",
    "$$\n",
    "\\begin{align}\n",
    "P(G) &= \\frac{e^{-H(G)}}{Z} = \\frac{e^{-\\theta\\bar{m}}}{(1 + e^{-\\theta})^{n \\choose 2}} \\\\\n",
    "                  &= p^\\bar{m}(1-p)^{{n \\choose 2} - \\bar{m}}\n",
    "\\end{align}              \n",
    "$$\n",
    "\n",
    "where $ p = \\frac{1}{1 + e^\\theta}$ ($ 1-p = \\frac{1}{1 + e^{-\\theta}})$\n",
    "\n",
    "\n",
    "This is the **Bernouilli Random Graph**, in which each of the edges has a probability p of existing, and the edges are sampled independently\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Degree of each edge\n",
    "One can also constrain the graph such that each vertex has a known degree. \n",
    "In this case the constraints are $$k_i(G) = \\sum_{j \\neq i} a_{ij}$$\n",
    "The corresponding observables are $\\bar{k_i}$.\n",
    "\n",
    "\n",
    "The Hamiltonian can thus be expressed as \n",
    "$$\n",
    "\\begin{align}\n",
    "H(G) &= \\sum_i \\theta_i \\sum _{j\\neq i} a_{ij}\\\\\n",
    "    &= \\sum_{i<j} (\\theta_i + \\theta_j) a_{ij}\n",
    "\\end{align}\n",
    "$$\n",
    "The last equality is obtained by the fact that the graph is unoriented: $a_{ij} = a_{ji}$\n",
    "\n",
    "The Partition function is \n",
    "$$\n",
    "\\begin{align}\n",
    "    Z &= \\sum_G e^{-H(G)} = \\sum_{A \\in \\{0,1\\}^{n^2}} exp(-\\sum_{i<j} (\\theta_i + \\theta_j) a_{ij}) \\\\  \n",
    "      &= \\prod_{i<j} \\sum_{a_{ij}\\in \\{0,1\\}} exp(-\\sum_{i<j} (\\theta_i + \\theta_j) a_{ij}) \\\\\n",
    "      &= \\prod_{i<j} (1 + e^{-\\theta_i + \\theta_j}) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this formula, one could consider a more general case where the hamiltonian would write as :\n",
    "$$\n",
    "H(G) = \\sum_{i<j} \\theta_{ij} a_{ij}\n",
    "$$\n",
    "\n",
    "This cas includes the total number of edges one, by taking a constant $\\theta_{ij}$m and the degree one by taking $\\theta_{ij} = \\theta_{i} + \\theta_{j}$\n",
    "\n",
    "Using this notation the free energy can be written as \n",
    "\n",
    "$$F = -\\sum_{i<j} ln(1 + exp(-\\theta_{ij}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the edge probabilities can be expressed as \n",
    "$$\n",
    "\\begin{align}\n",
    "p_{ij} &= \\frac{\\partial F}{\\partial \\theta_{ij}} \\\\\n",
    "        &= \\frac{1}{1 + e^{\\theta_{ij}}} \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
