{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An information theoretical introduction to KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulas\n",
    "\n",
    "- Entropy : $H(p) = - \\sum_{x \\in X} p(x) log(p(x))$\n",
    "\n",
    "\n",
    "- Cross-Entropy : $C(p|q) = - \\sum_{x \\in X} p(x) log(q(x))$\n",
    "\n",
    "\n",
    "- Kullback-Leibler Divergence : $KL(p|q) = \\sum_{x \\in X} p(x) log(\\frac{p(x)}{q(x)})$\n",
    "\n",
    "where does the $log$ come from, what do these quantities measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Claude Shannon](shannon.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information content and entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How much information is contained in the realization of an event?\n",
    "How surprising an event is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equiprobable case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image0](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\text{\"rain\"}) = 0.5 = \\frac{1}{2^1}$\n",
    "The level of uncertainty of the user is divided by 2, so he is provided with 1 bit of information.\n",
    "\n",
    "On average, the realization of an events provides the user with $- (0.5 \\times log_2(0,5) + 0.5 \\times log_2(0,5)) = 1$ bit. This is the entropy of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-equiprobable case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image1](image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent to a uniform distribution over the 4 events {\"sun\", \"sun\", \"sun\", \"rain\"} \n",
    "\n",
    "$P(\\text{\"rain\"}) = 0.25 = \\frac{1}{2^2}$\n",
    "\n",
    "The level of uncertainty of the user is divided by 4 -> 2 bits of information\n",
    "\n",
    "On average, the realization of an events provides the user with $- (0.75 \\times log_2(0,75) + 0.25 \\times log_2(0,25)) = 0.81$ bits $= H(p)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information Content** associated with an event $x$ under the probability $p$: $$I_p(x) = log(\\frac{1}{p(x)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent : minimum number of bits needed to encode the realization of the event.\n",
    "\n",
    "- **Bridge** between the worlds of probabilities and information theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average of the Information Content over all the possible events:\n",
    "$$H(p) = E_p(I_p(x)) = - \\sum_{x \\in X} p(x) log(p(x))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should I encode these events to communicate them in a channel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image1](image7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each event $x$ is encoded with a binary code of length $m(x)$. Here $m(x) = 3$ for all events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the event \"sun\" will occur more frequently so we should encode it on less bits than a rarer event such as \"storm\".\n",
    "Quantitatively, I could reduce the **average of my messages lengths** by changing the encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- $H(p) = 2.23$ bits\n",
    "- $C(p|q) = 3$ bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This average of message length under the **true distribution** is the **Cross Entropy** of my encoding.\n",
    "\n",
    "Cross-Entropy : $C(p|q) = E_p(m(x)) = E_p(I_q(x))$ where $q(x) = \\frac{1}{2^{m(x)}}$ is the distribution underlying the binary messages.\n",
    "\n",
    "\n",
    "Indeed choosing an encoding is equivalent to making assumptions on the events probabilities (here a uniform distribution, with probabilities equal to $\\frac{1}{2^3}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This encoding is Better: ![image1](image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C(p|q)=2.42 $ bits -> closest to the entropy than the first encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Entropy** : $$C(p|q) = - \\sum_{x \\in X} p(x) log(q(x))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback-Leibler divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much extra information do I send with my encoding? \n",
    "\n",
    "Instead of computing the message lengths for each message, we measure the difference between our message lengths and the information content under the true distribution $p$.\n",
    "$\\delta_{p,q}(x) = m(x)-I_p(x) = I_q(x)-I_p(x)$ \n",
    "\n",
    "The **Kullback-Leibler divergence** is the average of this additional message lengths:\n",
    "\n",
    "$$\\begin{align}\n",
    "KL(p|q) &= E_p[\\delta(x)] \\\\&= \\sum_{x \\in X} p(x) (log(\\frac{1}{q(x)}) - log(\\frac{1}{p(x)})) \\\\ &= \\sum_{x \\in X} p(x) log(\\frac{p(x)}{q(x)}) \\\\ &= - \\sum_{x \\in X} p(x) log(q(x)) - (- \\sum_{x \\in X} p(x) log(p(x)) \\\\ &= C(p|q) - H(p)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image1](image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, I send $K(p|q) = 2.42-2.23 = 0.19 $ more bits than necesary into my channel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link with Cross-Entropy \n",
    "$$C(p|q) = H(p) + KL(p|q) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multi-class classification $q$ will be a distribution over classes estimated from sample data.\n",
    "$$ agmin_q C(p|q) \\Leftrightarrow armin_q KL(p|q) $$\n",
    "\n",
    "$C(p|q)$ is easier to compute (no Entropy term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Largely inspired from the video \"A Short Introduction to Entropy, Cross-Entropy and KL-Divergence\", by Aurélien Géron\n",
    "- https://en.wikipedia.org/wiki/Information_content\n",
    "- https://en.wikipedia.org/wiki/Entropy_(information_theory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
